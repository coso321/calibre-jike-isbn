#!/usr/bin/env python
# vim:fileencoding=UTF-8:ts=4:sw=4:sta:et:sts=4:ai
# License: GPLv3 Copyright: 2011, Kovid Goyal <kovid at kovidgoyal.net>
from __future__ import absolute_import, division, print_function, unicode_literals

import hashlib
import re
import time

try:
    from queue import Empty, Queue
except ImportError:
    from Queue import Empty, Queue

from calibre import as_unicode
from calibre.ebooks.chardet import xml_to_unicode
from calibre.ebooks.metadata import check_isbn
from calibre.ebooks.metadata.book.base import Metadata
from calibre.ebooks.metadata.sources.base import Option, Source
from calibre.utils.cleantext import clean_ascii_chars
from calibre.utils.localization import canonicalize_lang

NAMESPACES = {
    "openSearch": "http://a9.com/-/spec/opensearchrss/1.0/",
    "atom": "http://www.w3.org/2005/Atom",
    "dc": "http://purl.org/dc/terms",
    "gd": "http://schemas.google.com/g/2005",
}


def get_details(browser, url, timeout):  # {{{
    try:
        raw = browser.open_novisit(url, timeout=timeout).read()
    except Exception as e:
        print("try here 1 ?")
        gc = getattr(e, "getcode", lambda: -1)
        if gc() != 403:
            raise
        # Google is throttling us, wait a little
        time.sleep(2)
        raw = browser.open_novisit(url, timeout=timeout).read()

    return raw


# }}}

xpath_cache = {}


def XPath(x):
    ans = xpath_cache.get(x)
    if ans is None:
        from lxml import etree

        ans = xpath_cache[x] = etree.XPath(x, namespaces=NAMESPACES)
    return ans


def cleanup_title(title):
    if ":" in title:
        return title.partition(":")[0]
    return re.sub(r"(.+?) \(.+\)", r"\1", title)


def get_isbn_url(isbns):
    try:
        from urllib.parse import urlencode
    except ImportError:
        from urllib import urlencode
    ISBN_URL = "https://api.jike.xyz/situ/book/isbn/"

    urls = []

    for isbn in isbns:
        print("isbn is ", isbn)
        isbn = isbn.strip()
        url = ISBN_URL + isbn
        urls.append(url)

    return urls


def to_metadata(browser, log, entry_, timeout):  # {{{
    from calibre.utils.date import parse_date, utcnow
    import re

    douban_id = str(entry_.get("douban"))
    log("douban_id", douban_id)
    title = entry_.get("name")
    log("title", title)
    description = entry_.get("description")
    # subtitle = entry_.get('subtitle')  # TODO: std metada doesn't have this field
    isbn = str(entry_.get("id"))  # ISBN11 is obsolute, use ISBN13
    cover_url = entry_.get("photoUrl")
    publisher = entry_.get("publishing")
    pubdate = entry_.get("published")
    authors = entry_.get("author")
    rating = entry_.get("doubanScore")

    if not authors:
        authors = [_("Unknown")]
    else:
        authors = [authors]
    if not douban_id or not title:
        # Silently discard this entry
        return None

    mi = Metadata(title, authors)
    mi.identifiers = {"douban": douban_id}
    mi.publisher = publisher
    mi.comments = description

    # ISBN
    isbns = []
    if isinstance(isbn, (type(""), bytes)):
        if check_isbn(isbn):
            isbns.append(isbn)
    else:
        for x in isbn:
            if check_isbn(x):
                isbns.append(x)
    if isbns:
        mi.isbn = sorted(isbns, key=len)[-1]
    mi.all_isbns = isbns

    # Ratings
    if rating:
        try:
            mi.rating = rating / 200.0
        except:
            log.exception("Failed to parse rating")
            mi.rating = 0

    # Cover
    mi.has_douban_cover = None
    u = cover_url
    if u:
        # If URL contains "book-default", the book doesn't have a cover
        if u.find("book-default") == -1:
            mi.has_douban_cover = u

    # pubdate
    if pubdate:
        try:
            default = utcnow().replace(day=15)
            mi.pubdate = parse_date(pubdate, assume_utc=True, default=default)
        except:
            log.error("Failed to parse pubdate %r" % pubdate)

    # Tags
    # mi.tags = tags

    ## pubdate
    # pubdate = get_text(extra, date)
    # if pubdate:
    #    from calibre.utils.date import parse_date, utcnow
    #    try:
    #        default = utcnow().replace(day=15)
    #        mi.pubdate = parse_date(pubdate, assume_utc=True, default=default)
    #    except:
    #        log.error('Failed to parse pubdate %r' % pubdate)

    return mi


def get_isbns(browser, log, entry_, timeout):  # {{{
    from lxml import etree

    entry = XPath("//atom:entry")
    entry_id = XPath("descendant::atom:id")
    url = XPath('descendant::atom:link[@rel="self"]/@href')
    creator = XPath("descendant::dc:creator")
    identifier = XPath("descendant::dc:identifier")
    title = XPath("descendant::dc:title")
    date = XPath("descendant::dc:date")
    publisher = XPath("descendant::dc:publisher")
    subject = XPath("descendant::dc:subject")
    description = XPath("descendant::dc:description")
    language = XPath("descendant::dc:language")

    id_url = entry_id(entry_)[0].text
    google_id = id_url.split("/")[-1]
    details_url = url(entry_)[0]
    title_ = ": ".join([x.text for x in title(entry_)]).strip()
    authors = [x.text.strip() for x in creator(entry_) if x.text]

    # ISBN
    isbns = []

    try:
        raw = get_details(browser, details_url, timeout)
        feed = etree.fromstring(
            xml_to_unicode(clean_ascii_chars(raw), strip_encoding_pats=True)[0],
            parser=etree.XMLParser(
                recover=True, no_network=True, resolve_entities=False
            ),
        )
        extra = entry(feed)[0]
    except:
        log.exception("Failed to get additional details for")
        return None

    for x in identifier(extra):
        t = type("")(x.text).strip()
        if t[:5].upper() in ("ISBN:", "LCCN:", "OCLC:"):
            if t[:5].upper() == "ISBN:":
                t = check_isbn(t[5:])
                if len(t) == 13:
                    isbns.append(t)
    return isbns


# }}}


class jike(Source):

    name = "jike isbn"
    author = "Rlyehzoo"
    version = (1, 1, 0)
    minimum_calibre_version = (1, 0, 0)

    description = _(
        "Downloads metadata and covers based on https://jike.xyz/api/isbn.html"
        "Useful only for Chinese language books."
    )

    capabilities = frozenset({"identify", "cover"})
    touched_fields = frozenset(
        {
            "title",
            "authors",
            "tags",
            "pubdate",
            "comments",
            "publisher",
            "identifier:isbn",
            "identifier:douban",
        }
    )
    supports_gzip_transfer_encoding = True
    cached_cover_url_is_reliable = False

    DOUBAN_API_URL = "https://api.douban.com/v2/book/search"
    DOUBAN_BOOK_URL = "https://book.douban.com/subject/%s/"
    GOOGLE_COVER = "https://books.google.com/books?id=%s&printsec=frontcover&img=1"

    options = (
        Option(
            "include_subtitle_in_title",
            "bool",
            True,
            _("Include subtitle in book title:"),
            _("Whether to append subtitle in the book title."),
        ),
        Option(
            "apikey", "string", "", _("zhujian api apikey"), _("zhujian api apikey")
        ),
    )

    def get_book_url(self, identifiers):  # {{{
        db = identifiers.get("douban", None)
        if db is not None:
            return ("douban", db, self.DOUBAN_BOOK_URL % db)

    # }}}

    def create_query(
        self, log, apikey, title=None, authors=None, identifiers={}
    ):  # {{{
        try:
            from urllib.parse import urlencode
        except ImportError:
            from urllib import urlencode
        BASE_URL = "https://books.google.com/books/feeds/volumes?"
        ISBN_URL = "https://api.jike.xyz/situ/book/isbn/"

        isbn = check_isbn(identifiers.get("isbn", None))
        q = ""
        t = None
        if isbn is not None:
            q = isbn
            t = "isbn"
        elif title or authors:

            authors = None

            def build_term(prefix, parts):
                return " ".join("in" + prefix + ":" + x for x in parts)

            title_tokens = list(self.get_title_tokens(title))
            if title_tokens:
                q += build_term("title", title_tokens)
            author_tokens = list(
                self.get_author_tokens(authors, only_first_author=True)
            )
            if author_tokens:
                q += ("+" if q else "") + build_term("author", author_tokens)

        if not q:
            return None

        url = None

        if t == "isbn":
            url = "{}/{}?apikey={}".format(ISBN_URL, q, apikey)
            return url
        else:
            if not isinstance(q, bytes):
                q = q.encode("utf-8")
            return BASE_URL + urlencode(
                {
                    "q": q,
                    "max-results": 20,
                    "start-index": 1,
                    "min-viewability": "none",
                }
            )

    # }}}

    def download_cover(  # {{{
        self,
        log,
        result_queue,
        abort,
        title=None,
        authors=None,
        identifiers={},
        timeout=30,
        get_best_cover=False,
    ):
        cached_url = self.get_cached_cover_url(identifiers)
        if cached_url is None:
            log.info("No cached cover found, running identify")
            rq = Queue()
            self.identify(
                log, rq, abort, title=title, authors=authors, identifiers=identifiers
            )
            if abort.is_set():
                return
            results = []
            while True:
                try:
                    results.append(rq.get_nowait())
                except Empty:
                    break
            results.sort(
                key=self.identify_results_keygen(
                    title=title, authors=authors, identifiers=identifiers
                )
            )
            for mi in results:
                cached_url = self.get_cached_cover_url(mi.identifiers)
                if cached_url is not None:
                    break
        if cached_url is None:
            log.info("No cover found")
            return

        br = self.browser

        log("Downloading cover from:", cached_url)

        try:
            cdata = br.open_novisit(cached_url, timeout=timeout).read()
            if cdata:
                result_queue.put((self, cdata))
        except:
            log.exception("Failed to download cover from:", cached_url)

    #        for candidate in (0, 1):
    #            if abort.is_set():
    #                return
    #            url = cached_url + '&zoom={}'.format(candidate)
    #            log('Downloading cover from:', cached_url)
    #            try:
    #                cdata = br.open_novisit(url, timeout=timeout).read()
    #                if cdata:
    #                    if hashlib.md5(cdata).hexdigest() in self.DUMMY_IMAGE_MD5:
    #                        log.warning('Google returned a dummy image, ignoring')
    #                    else:
    #                        result_queue.put((self, cdata))
    #                        break
    #            except Exception:
    #                log.exception('Failed to download cover from:', cached_url)

    # }}}

    def get_cached_cover_url(self, identifiers):  # {{{
        url = None
        db = identifiers.get("douban", None)
        if db is None:
            isbn = identifiers.get("isbn", None)
            if isbn is not None:
                db = self.cached_isbn_to_identifier(isbn)
        if db is not None:
            url = self.cached_identifier_to_cover_url(db)

        return url

    # }}}

    def get_all_details(self, br, log, entries, abort, result_queue, timeout):  # {{{
        from lxml import etree

        for relevance, i in enumerate(entries):
            try:
                ans = to_metadata(br, log, i, timeout)
                if isinstance(ans, Metadata):
                    ans.source_relevance = relevance
                    db = ans.identifiers["douban"]
                    for isbn in getattr(ans, "all_isbns", []):
                        self.cache_isbn_to_identifier(isbn, db)
                    if ans.has_douban_cover:
                        self.cache_identifier_to_cover_url(db, ans.has_douban_cover)
                    self.clean_downloaded_metadata(ans)
                    result_queue.put(ans)
            except:
                log.exception(
                    "Failed to get metadata for identify entry:", etree.tostring(i)
                )
            if abort.is_set():
                break

    # }}}

    def identify(  # {{{
        self,
        log,
        result_queue,
        abort,
        title=None,
        authors=None,
        identifiers={},
        timeout=30,
    ):
        from lxml import etree
        import json

        entry = XPath("//atom:entry")

        # check apikey
        if not self.prefs.get("apikey"):
            return

        query = self.create_query(
            log,
            apikey=self.prefs["apikey"],
            title=title,
            authors=authors,
            identifiers=identifiers,
        )
        if not query:
            log.error("Insufficient metadata to construct query")
            return
        isbn = check_isbn(identifiers.get("isbn", None))
        log("Making query:", query)
        br = self.browser
        if isbn is not None:
            # br.addheaders = [
            #    ('apikey', self.prefs["apikey"]),
            # ]

            try:
                raw = br.open_novisit(query, timeout=timeout).read()
            except Exception as e:
                log.exception("Failed to make identify query: %r" % query)
                return as_unicode(e)
            try:
                j = json.loads(raw)
            except Exception as e:
                log.exception("Failed to parse identify results")
                return as_unicode(e)
            if "data" in j:
                j = j["data"]
                entries = [j]
            else:
                entries = []
                entries.append(j)
            if not entries and identifiers and title and authors and not abort.is_set():
                return self.identify(
                    log,
                    result_queue,
                    abort,
                    title=title,
                    authors=authors,
                    timeout=timeout,
                )
            log("here entries are", entries)
        else:
            try:
                raw = br.open_novisit(query, timeout=timeout).read()
            except Exception as e:
                log.exception("Failed to make identify query: %r" % query)
                return as_unicode(e)

            try:
                feed = etree.fromstring(
                    xml_to_unicode(clean_ascii_chars(raw), strip_encoding_pats=True)[0],
                    parser=etree.XMLParser(
                        recover=True, no_network=True, resolve_entities=False
                    ),
                )
                entries = entry(feed)
            except Exception as e:
                log.exception("Failed to parse identify results")
                return as_unicode(e)

            if not entries and title and not abort.is_set():
                if identifiers:
                    log("No results found, retrying without identifiers")
                    return self.identify(
                        log,
                        result_queue,
                        abort,
                        title=title,
                        authors=authors,
                        timeout=timeout,
                    )
                ntitle = cleanup_title(title)
                if ntitle and ntitle != title:
                    log("No results found, retrying without sub-title")
                    return self.identify(
                        log,
                        result_queue,
                        abort,
                        title=ntitle,
                        authors=authors,
                        timeout=timeout,
                    )

            # self.get_all_details(br, log, entries, abort, result_queue, timeout)
            isbns = []
            for relevance, i in enumerate(entries):
                isbn_i = get_isbns(br, log, i, timeout)
                time.sleep(2)
                isbns.extend(isbn_i)

            queries = get_isbn_url(isbns)

            entries = None

            for query in queries:
                br = self.browser
                br.addheaders = [
                    ("apikey", self.prefs["apikey"]),
                ]
                log("apikey is ", self.prefs["apikey"])
                try:
                    raw = br.open_novisit(query, timeout=timeout).read()
                except Exception as e:
                    break
                try:
                    j = json.loads(raw)
                except Exception as e:
                    log.exception("Failed to parse identify results")
                    return as_unicode(e)
                if "data" in j:
                    j = j["data"]
                if j is not None:
                    if entries is None:
                        # log('here111')
                        entries = [j]
                    else:
                        # log('here222')
                        # log('type0',type(entries))
                        entry = [j]
                        new = entries + entry
                        entries = new
                        # log('type1',type(j))
                        # log('type2',type([j]))
                        # log('type2.5',type(entry))
                        # log('type2.2',type(new))
                        # log('type3',type(entries))
                # log('this entry is', entries)

                if (
                    not entries
                    and identifiers
                    and title
                    and authors
                    and not abort.is_set()
                ):
                    return self.identify(
                        log,
                        result_queue,
                        abort,
                        title=title,
                        authors=authors,
                        timeout=timeout,
                    )
                # There is no point running these queries in threads as douban
                # throttles requests returning 403 Forbidden errors
        self.get_all_details(br, log, entries, abort, result_queue, timeout)

    # }}}


if __name__ == "__main__":  # tests {{{
    # To run these test use: calibre-debug
    # src/calibre/ebooks/metadata/sources/google.py
    from calibre.ebooks.metadata.sources.test import (
        test_identify_plugin,
        title_test,
        authors_test,
    )

    tests = [
        # (
        #       {
        #           "identifiers": {"isbn": " 9787542663764"},
        #           "title": "八月炮火",
        #           "authors": ["巴巴拉·塔奇曼"],
        #       },
        #       [title_test("八月炮火", exact=True), authors_test(["巴巴拉·塔奇曼"])],
        #   ),
        ({"title": "新名字的故事"}, [title_test("新名字的故事", exact=False)])
    ]
    test_identify_plugin(jike.name, tests[:])

# }}}
